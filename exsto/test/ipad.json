{"date": "2014-11-01T16:36:35+00:00", "id": "4566CED7-C0DF-4AA5-863D-F4784A99067B", "next_thread": "CAA_qdLq4ei7tzSpmWEUBMXaJYGJ7KYzNA4oNKV+a9QdbHVrjkA", "next_url": "http://mail-archives.apache.org/mod_mbox/spark-user/201411.mbox/%3cCALEj8eMRR2dO4Gmdor-4FOYZR8V6yFEtkha-Yo0sRPF70Xn5Fg@mail.gmail.com%3e", "prev_thread": "CAA_qdLqJzUPkiFkSNRLrpUBOFqCnWLLAbQuH4kbK1A956MN8bA", "sender": "Jean-Pascal Billaud ...@tellapart.com>", "subject": "Re: SparkSQL + Hive Cached Table Exception", "text": "\nGreat! Thanks.\n\nSent from my iPad\n\n> On Nov 1, 2014, at 8:35 AM, Cheng Lian <lian.cs.zju@gmail.com> wrote:\n> \n> Hi Jean,\n> \n> Thanks for reporting this. This is indeed a bug: some column types (Binary, Array, Map\nand Struct, and unfortunately for some reason, Boolean), a NoopColumnStats is used to collect\ncolumn statistics, which causes this issue. Filed SPARK-4182 to track this issue, will fix\nthis ASAP.\n> \n> Cheng\n> \n>> On Fri, Oct 31, 2014 at 7:04 AM, Jean-Pascal Billaud <jp@tellapart.com> wrote:\n>> Hi,\n>> \n>> While testing SparkSQL on top of our Hive metastore, I am getting some java.lang.ArrayIndexOutOfBoundsException\nwhile reusing a cached RDD table.\n>> \n>> Basically, I have a table \"mtable\" partitioned by some \"date\" field in hive and below\nis the scala code I am running in spark-shell:\n>> \n>> val sqlContext = new org.apache.spark.sql.hive.HiveContext(sc);\n>> val rdd_mtable = sqlContext.sql(\"select * from mtable where date=20141028\");\n>> rdd_mtable.registerTempTable(\"rdd_mtable\");\n>> sqlContext.cacheTable(\"rdd_mtable\");\n>> sqlContext.sql(\"select count(*) from rdd_mtable\").collect(); <-- OK\n>> sqlContext.sql(\"select count(*) from rdd_mtable\").collect(); <-- Exception\n>> \n>> So the first collect() is working just fine, however running the second collect()\nwhich I expect use the cached RDD throws some java.lang.ArrayIndexOutOfBoundsException, see\nthe backtrace at the end of this email. It seems the columnar traversal is crashing for some\nreasons. FYI, I am using spark ToT (234de9232bcfa212317a8073c4a82c3863b36b14).\n>> \n>> java.lang.ArrayIndexOutOfBoundsException: 14\n>> \tat org.apache.spark.sql.catalyst.expressions.GenericRow.apply(Row.scala:142)\n>> \tat org.apache.spark.sql.catalyst.expressions.BoundReference.eval(BoundAttribute.scala:37)\n>> \tat org.apache.spark.sql.catalyst.expressions.Expression.n2(Expression.scala:108)\n>> \tat org.apache.spark.sql.catalyst.expressions.Add.eval(arithmetic.scala:89)\n>> \tat org.apache.spark.sql.columnar.InMemoryRelation$$anonfun$computeSizeInBytes$1.apply(InMemoryColumnarTableScan.scala:66)\n>> \tat org.apache.spark.sql.columnar.InMemoryRelation$$anonfun$computeSizeInBytes$1.apply(InMemoryColumnarTableScan.scala:66)\n>> \tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n>> \tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n>> \tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n>> \tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n>> \tat scala.collection.TraversableLike$class.map(TraversableLike.scala:244)\n>> \tat scala.collection.AbstractTraversable.map(Traversable.scala:105)\n>> \tat org.apache.spark.sql.columnar.InMemoryRelation.computeSizeInBytes(InMemoryColumnarTableScan.scala:66)\n>> \tat org.apache.spark.sql.columnar.InMemoryRelation.statistics(InMemoryColumnarTableScan.scala:87)\n>> \tat org.apache.spark.sql.columnar.InMemoryRelation.statisticsToBePropagated(InMemoryColumnarTableScan.scala:73)\n>> \tat org.apache.spark.sql.columnar.InMemoryRelation.withOutput(InMemoryColumnarTableScan.scala:147)\n>> \tat org.apache.spark.sql.CacheManager$$anonfun$useCachedData$1$$anonfun$applyOrElse$1.apply(CacheManager.scala:122)\n>> \tat org.apache.spark.sql.CacheManager$$anonfun$useCachedData$1$$anonfun$applyOrElse$1.apply(CacheManager.scala:122)\n>> \tat scala.Option.map(Option.scala:145)\n>> \tat org.apache.spark.sql.CacheManager$$anonfun$useCachedData$1.applyOrElse(CacheManager.scala:122)\n>> \tat org.apache.spark.sql.CacheManager$$anonfun$useCachedData$1.applyOrElse(CacheManager.scala:119)\n>> \tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:144)\n>> \tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:162)\n>> \tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n>> \tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n>> \tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n>> \tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)\n>> \tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)\n>> \tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)\n>> \tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)\n>> \tat scala.collection.AbstractIterator.to(Iterator.scala:1157)\n>> \tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)\n>> \tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)\n>> \tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)\n>> \tat scala.collection.AbstractIterator.toArray(Iterator.scala:1157)\n>> \tat org.apache.spark.sql.catalyst.trees.TreeNode.transformChildrenDown(TreeNode.scala:191)\n>> \tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:147)\n>> \tat org.apache.spark.sql.CacheManager$class.useCachedData(CacheManager.scala:119)\n>> \tat org.apache.spark.sql.SQLContext.useCachedData(SQLContext.scala:49)\n>> \tat org.apache.spark.sql.SQLContext$QueryExecution.withCachedData$lzycompute(SQLContext.scala:376)\n>> \tat org.apache.spark.sql.SQLContext$QueryExecution.withCachedData(SQLContext.scala:376)\n>> \tat org.apache.spark.sql.SQLContext$QueryExecution.optimizedPlan$lzycompute(SQLContext.scala:377)\n>> \tat org.apache.spark.sql.SQLContext$QueryExecution.optimizedPlan(SQLContext.scala:377)\n>> \tat org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan$lzycompute(SQLContext.scala:382)\n>> \tat org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan(SQLContext.scala:380)\n>> \tat org.apache.spark.sql.SQLContext$QueryExecution.executedPlan$lzycompute(SQLContext.scala:386)\n>> \tat org.apache.spark.sql.SQLContext$QueryExecution.executedPlan(SQLContext.scala:386)\n>> \n>> Thanks,\n> \n\n"}
